---
title: 'Worksheet 6: Text Analysis'
author: 'Candy Boatwright'
date: 'April 3, 2024'
---

_This is the sixth in a series of worksheets for History 8510 at Clemson University. The goal of these worksheets is simple: practice, practice, practice. The worksheet introduces concepts and techniques and includes prompts for you to practice in this interactive document. When you are finished, you should change the author name (above), knit your document, and upload it to canvas. Don't forget to commit your changes as you go and push to github when you finish the worksheet._

Text analysis is an umbrella for a number of different methodologies. Generally speaking, it involves taking a set (or corpus) of textual sources, turning them into data that a computer can understand, and then running calculations and algorithms using that data. Typically, at its most basic level, that involves the counting of words.

**Text analysis can be broken down into 4 general steps:** 

  1. Acquiring a corpus
  2. Preparing the text or Pre-processing
  3. Choosing an analytical tool 
    * (There are many different tools or methods for text analysis. Take a minute and Google each of these methodologies: tf-idf, topic modeling, sentiment analysis, word vector analysis, n-grams)
  4. Analyzing the results
  
In this worksheet we are focusing on basic text analysis. We'll learn how to load textual data into R, how to prepare it, and then how to analyze it using tf-idf or term-frequency according to inverse document frequency. 

Before doing too much, lets load a few relevant libraries. The last few you will likely need to install.
```{r message=FALSE, warning=FALSE}
library(tidyverse)
library(tidytext) 
library(readtext)
library(widyr)
library(SnowballC)
```


## Acquiring a Corpus

First, lets install the State of the Union package. This package contains text of all the state of the Union addresses from Washington to Trump. Run `install.packages` to install the `sotu` package. 
```{r}
library(sotu)
```

This package includes both the metadata about these speeches in `sotu_meta` and the texts themselves in `sotu_texts`. Lets first look at the metadata associated with this package. 

```{r}
meta <- as.data.frame(sotu_meta)
head(meta)
```

This package also includes a function that will let us write all of the files to disk. This is crucial but also an unusual step because when conducting text analysis in the real world, you will not have an R package filled with the data. Rather you will have to organize the metadata and load the files yourself. Writing these to the disk allows us to practice that step. 

```{r}
file_paths <- sotu_dir(dir = "sotu_files")
head(file_paths)
```

What this does is create a new directory (sotu_files) and adds each State of the Union address as a text file. Notice each speech is its own .txt file that is comprised of just the text of the speech.

(@) Take a look at the directory in your files pane and open one of the documents. 

Now lets load all these texts into R using the `readtext()` function. First look up the documentation for this function and read about it. 
```{r}
sotu_texts <- readtext(file_paths)
```

Take a look at sotu_texts now. Notice that we have two columns, one filled with the text, and one with a document id. 
```{r}
head(sotu_texts, n = 5)
```

Now our textual data is loaded into R but the textual data and the metadata are in two different data frames. Lets combine them. Note that this isn't the way I would typically recommend doing this but its a quirk of the SOTU data. Typically when I create a metadata spreadsheet for a textual dataset I have a column for the file name which makes joining the textual data and metadata together easier. Here, we'll need to sort the dataset so that is alphabetical and then join the two together.

```{r}
sotu_whole <- 
  sotu_meta %>%  
  arrange(president) %>% # sort metadata
  bind_cols(sotu_texts) %>% # combine with texts
  as_tibble() # convert to tibble for better screen viewing

glimpse(sotu_whole)
```

Now our data is loaded into R and its ready to be pre-processed. 

## Pre-Processing 

### Tokenizing

One of the most basic pre-processing techniques for textual data is to tokenize it. Tokenization is essentially splitting a phrase, sentence, paragraph, or an entire text document into smaller units, such as individual words or terms. Each of these smaller units are called tokens. The tokens could be words, numbers or punctuation marks but, for historians, its common to remove the numbers and punctuation too. To do this we'll create a data frame where each row contains a single word with its metadata as unit of observation.

`tidytext` provides a function called `unnest_tokens().` We can use this to convert our sotu_whole data frame into one that is tokenized. It takes three arguments:
    
    * a tibble or data frame which contains the text
    * the name of the newly created column that will contain the tokens
    * the name of the column within the data frame which contains the text to be tokenized

```{r}
tidy_sotu <- sotu_whole %>%
  unnest_tokens(word, text)

tidy_sotu
```

`unnest_tokens()` also did something else that is really important: it made everything lowercase and took out all punctuation. The function contains options if we wanted to keep those elements, but for our purposes we don't. 

The function `unnest_tokens()` also has an option called token. Tokenizing by word is the default but you could also tokenize by characters, ngrams, lines, or sentences. 

(@)Use the documentation to tokenize the dataset into sentences: 
```{r}
tidy_sotu_sentences <- sotu_whole %>%
  unnest_tokens(sentence, text, token = "sentences")

tidy_sotu_sentences
```

We've talked about n-grams loosely in class. But lets define it more formally. An n-gram is a contiguous sequence of n items from a given sample of text or speech. The n stands for the number of items. So for example, a bi-gram is sets of two words. 

For example, if I had the string: "Nothing to fear but fear itself" A bi-gram would look like this: 
  Nothing to, to fear, fear but, but fear, fear itself.

A tri-gram would look like this: 
  Nothing to fear, to fear but, but fear itself
  
We can use unnest_tokens() to create n-grams for us. To do that we just have to add an extra option that defines n. 
```{r}
sotu_bigrams <- sotu_whole %>%
  unnest_tokens(bigram, text, token = "ngrams", n = 2)

head(sotu_bigrams$bigram)
```

(@) Use `unest_tokens()` to create tri-grams. 
```{r}
sotu_trigrams <- sotu_whole %>%
  unnest_tokens(trigram, text, token = "ngrams", n = 3)

head(sotu_trigrams$trigram)
```

### Stopwords

Another crucial component of text analysis is removing stopwords. Stopwords are words like "I, he, she, of, the" that are common and don't convey meaning. Because they are highly common they don't tell us anything about the content of the text itself. 

There are stopwords that come with the `tidytext` package. 
```{r}
stop_words
```
This is just one example of stopwords. You can find other lists such as stopwords in other languages or [stopwords designed specifically for the 19th century.](https://www.matthewjockers.net/macroanalysisbook/expanded-stopwords-list/) Its also possible you may want to edit the list of stopwords to include some of your own. For example, if we wanted to add the word, "America" to the stopwords list we could use add_row to do so: 
```{r}
stop_words_custom <- stop_words %>% add_row(word="America", lexicon="NA")
```


For now lets just remove the default stopwords. The easiest way to do that here is to do an anti-join. We join and return all rows from our table of tokens tidy_sotu where there are no matching values in our list of stopwords. 

```{r}
tidy_sotu_words <- tidy_sotu %>% 
  anti_join(stop_words)

tidy_sotu_words

#another way to do this would be to filter by words NOT in the stop word list like this:  filter(!word %in% stop_words$word)
```

### Stemming 

The third common kind of pre-process is called word stemming. This process reduces a word to its root stem. So for example: fishing becomes fish, fished becomes fish, fishes becomes fish. You can easily see how this might be useful for capturing all forms of a word.

`tidytext` doesn't have its own word stemming function. Instead we have to rely on the functions provided by `hunspell` or `SnowballC`. I prefer `SnowballC`. You may need to install it before running the below code. 

```{r}
library(SnowballC)
tidy_sotu_words %>%
        mutate(word_stem = wordStem(word))
```

Now if you compare the word and word_stem columns you can see the effect that wordStem had. Notice that it works well in cases like 
  
  citizens = citizen 

But it does some odd things to words like representatives. Whether this is useful for you will depend on the question your asking (and the OCR accuracy) but its a useful technique to be familiar with nevertheless. 

## Analysis

Lets reset our work space and ensure that our df is loaded with single tokenized words and filter by our stopword list. Go ahead and clear your environment using the broom button and then run the below code. This code is simply everything we've run up to this point. 

```{r}
meta <- as.data.frame(sotu_meta)
file_paths <- sotu_dir(dir = "sotu_files")
sotu_texts <- readtext(file_paths)
sotu_whole <- 
  sotu_meta %>%  
  arrange(president) %>% # sort metadata
  bind_cols(sotu_texts) %>% # combine with texts
  as_tibble() 

tidy_sotu <- sotu_whole %>%
  unnest_tokens(word, text) %>% 
  anti_join(stop_words)
```
(@) Before we move forward, take a minute a describe the chunk of code you just ran. What does each section do and how does it reflect the workflow for a topic modeling project? What are the important steps that are unique to topic modeling? 

> Creates the meta dataframe by converting the sotu_meta into a dataframe. 
Creates the file_paths directory and fills it with the sotu_files. 
Uses readtext to read the sotu_files (from file_paths) and assigns them to sotu_texts. 
Arranges sotu_meta by president and binds with sotu_texts and then converts to a tibble (dataframe) called sotu_whole. Not sure why this code block uses sotu_meta and not meta?
Creates the tidy_sotu dataframe from sotu_whole, unnests the speeches by word and removes standard stop words.

The most basic kind of analysis we might be interested in doing is counting words. We can do that easily using the `count()` function: 
```{r}
tidy_sotu %>%
  count(word, sort = TRUE)
```

Now we know that the most used word in state of the union speeches is government. But what if we wanted to look at when presidents use the words war versus the word peace? 
```{r}
tidy_sotu %>%
  filter(word %in% c("war", "peace")) %>% 
  count(year, word)
```

This data frame is to big to understand quickly without visualizing it. We can create a bar chart to better understand it: 
```{r}
library(ggplot2)

tidy_sotu %>%
  filter(word %in% c("war", "peace")) %>% 
  count(year, word) %>% 
  ggplot(aes(year, n, fill = word)) +
    geom_col(position = "fill")

```

We also might want to ask about the average length of each president's state of the union address. Who had the longest speech and who had the shortest?
```{r}
tidy_sotu %>%
  count(president, doc_id)  %>% 
  group_by(president) %>% 
  summarize(avg_words = mean(n)) %>% 
  arrange(desc(avg_words))
```

(@) Think back to the metadata that we loaded about these speeches. Why are more modern president's state of the union addresses shorter? 

> Because more of them are given in a speech form as opposed to written addresses that were more common earlier on. 

(@) Filter the dataset to address this discrepancy and the recreate these statistics: 
```{r}
tidy_sotu %>%
  filter(sotu_type == "speech") %>%
  count(president, doc_id)  %>% 
  group_by(president) %>% 
  summarize(avg_words = mean(n)) %>% 
  arrange(desc(avg_words))
```

### Term Frequency
Often, the raw frequency of a term is not as useful as relative frequency. In other words, how often that word appears relative to the total number of words in a text. This ratio is called **term frequency**. 

You can calculate the term frequency by dividing the total occurrences of a word by the total number of words. Typically you want to do this per document.

Here's an easy way to calculate it: 
```{r}
tidy_sotu_rel.freq <- tidy_sotu %>%
  count(doc_id, word, sort = T)  %>%# count occurrence of word and sort descending
  group_by(doc_id) %>% 
  mutate(n_tot = sum(n), # count total number of words per doc
         term_freq = n/n_tot)
head(tidy_sotu_rel.freq)
```

We can assume that words with a high frequency in the text are more important or significant. Here we can find the words with the most significance for each president: 
```{r}
tidy_sotu %>%
  count(president, word)  %>%  # count n for each word
  group_by(president) %>% 
  mutate(n_tot = sum(n), # count total number of words per doc
         term_freq = n/n_tot) %>% 
  arrange(desc(term_freq)) %>% # sort by term frequency
  top_n(1) %>%  # take the top for each president
  print(n = Inf) # print all rows
```
(@) The code above is commented to help you follow it. Walk through the code above, and explain what each line does in your own words. If its a function you are unfamiliar with, look up the documentation.

> Passes the tidy_sotu dataframe to the count function which counts the number of words by the president and then passes those counts to group_by to be grouped by the president.
It then uses the mutate function to create two new columns (n_tot) which is the total count of words grouped by president and term_freq which is the term frequency over the course of all that president's speeches.
Passes to arrange function which sorts term_freq descending.
Passes to top_n which returns the highest value in a column. Note in help documentation that this function is being retired and that they recommend using slice_max().
The print(n = Inf) prints every row of the tibble.

### TF-IDF

The above measures the frequency of terms within individual documents. But what if we know about words that seem more important based on the contents of the **entire** corpus? That is where tf-idf or term-frequency according to inverse document frequency comes in. 

Tf-idf measures how important a word is within a corpus by scaling term frequency per document according to the inverse of the term’s document frequency (number of documents within the corpus in which the term appears divided by the number of documents). The tf–idf value increases proportionally to the number of times a word appears in the document and is offset by the number of documents in the corpus that contain the word, which helps to adjust for the fact that some words appear more frequently in general. 

Don't worry too much about how tf-idf is calculated. But if you feel like you are a bit lost and want to understand the specifics - I recommend reading the [tf-idf wikipedia page](https://en.wikipedia.org/wiki/Tf%E2%80%93idf) and this blog post from [_Learn Data Science_](https://www.learndatasci.com/glossary/tf-idf-term-frequency-inverse-document-frequency/).

We'll calculate tf-idf in the next code chunk but lets talk for a second about what that number will represent. It will be: 

    * lower for words that appear frequently in many documents of the corpus, and lowest              when the word occurs in virtually all documents.
    * higher for words that appear frequently in just a few documents of the corpus,                  this lending high discriminatory power to those few documents.

Luckily, `tidytext` provides a function for calculating tf-idf. To calculate tf-idf the function needs a list of every word in every document and the count. Like this: 
```{r}
tidy_sotu %>%
  count(doc_id, word, sort = TRUE)  
```
We can feed that to the function and get the tf-idf: 
```{r}
sotu.tf.idf <- tidy_sotu %>%
  count(doc_id, word, sort = TRUE)  %>%  
  bind_tf_idf(word, doc_id, n) 

head(sotu.tf.idf)
```

The resulting data frame has 3 columns: term frequency (tf), inverse document frequency (idf) and Tf-idf (tf_idf).

Lets look at what the words with the highest tf-idf score are. 
```{r}
sotu.tf.idf %>% arrange(desc(tf_idf))
```

(@) Pick a president who served more than one term. Filter the dataset and generate both raw word counts and tf-idf scores for that president. What words are most significant in each method? Why and what does that tell you about that president? 

```{r}
#tidy_sotu %>%
#  count(doc_id, word, sort = TRUE)

andrew.jackson.tf.idf <- tidy_sotu %>%
  filter(president == "Andrew Jackson") %>%
  count(doc_id, word, sort = TRUE)  %>%
  group_by(word) %>%
  bind_tf_idf(word, doc_id, n) 

print(andrew.jackson.tf.idf)
```

### Co-Occurance
Co-occurrence gives us a sense of words that appear in the same text, but not necessarily next to each other. It shows words that are likely to co-occur. Note that this is different than topic modeling, which we'll discuss next week. 

For this section we will make use of the `widyr` package. The function which helps us do this is the `pairwise_count()` function. It lets us count common pairs of words co-appearing within the same speech. This function might take a second as the resulting data frame will be incredibly large.

```{r}
#| eval: false
sotu_word_pairs <- sotu_whole %>% 
  mutate(speech_end = word(text, -5000, end = -1)) %>%  # extract last 100 words
  unnest_tokens(word, speech_end) %>%   # tokenize
  filter(!word %in% stop_words$word) %>%  # remove stopwords
  pairwise_count(word, doc_id, sort = TRUE, upper = FALSE) # don't include upper triangle of matrix

head(sotu_word_pairs)
```

Now we have a list of words that appear near each other in the text as well as the frequency. Once again this dataset is far too large to look at in a data frame. Instead, we'll create a network graph that shows us the relationships between words for any words that appear more than 200 times. I chose 200 after looking at the above dataset and seeing that the highest count was 239. You want the network graph to be manageable and not too large. 
```{r}
#| eval: false
library(igraph)
library(ggraph)

sotu_word_pairs %>% 
  filter(n >= 200) %>%  # only word pairs that occur 200 or more times
  graph_from_data_frame() %>% #convert to graph
  ggraph(layout = "fr") + # place nodes according to the force-directed algorithm of Fruchterman and Reingold
  geom_edge_link(aes(edge_alpha = n, edge_width = n), edge_colour = "tomato") +
  geom_node_point(size = 5) +
  geom_node_text(aes(label = name), repel = TRUE, 
                 point.padding = unit(0.2, "lines")) +
  theme_void()
```
(@) Create a network graph that shows the relationship between words that appear between 125 and 175 times.

```{r}
#| eval: false
library(igraph)
library(ggraph)

sotu_word_pairs %>% 
  filter(n >= 125 & n <= 175) %>%  # only word pairs that occur between 125-175 times
  graph_from_data_frame() %>% 
  ggraph(layout = "fr") + 
  geom_edge_link(aes(edge_alpha = n, edge_width = n), edge_colour = "blue") +
  geom_node_point(size = 5) +
  geom_node_text(aes(label = name), repel = TRUE, 
                 point.padding = unit(0.2, "lines")) +
  theme_void()
```

## Analyzing Historical Newspapers About Buffalo Bill

To practice text analysis this week you will be helping Dr. Seefeldt with his research. In the github repository below I have included a metadata file containing all of the associated info from each article and a txt folder with each text file. 

The dataset is described by Dr. Seefeldt this way: 
> The project staff identified 99 visits to Indiana communities, some of which included performances spanning two or more days.  We located and captured a total of 720 Indiana newspaper pieces, including 273 articles promoting specific Wild West performances, 74 substantive pieces documenting locally-based reception to these performances, and 373 extraneous items, including advertisements, images, and reprints of articles generated elsewhere.  In total, the team transcribed approximately 740 items (R = Reception; P = Promotion; E = Extraneous)

Use this link to read more about the project and the historical context: [https://docs.google.com/document/d/1LnmyCob5zc984EkMAx3sXlXQqhNbm9O8XV4O3HrQUbQ/edit?usp=sharing](https://docs.google.com/document/d/1LnmyCob5zc984EkMAx3sXlXQqhNbm9O8XV4O3HrQUbQ/edit?usp=sharing)

You should use the rest of this worksheet to practice text analysis (and R more broadly) by exploring this dataset. Using the metadata provided and the raw text files can you use what you learned above to analyze these texts? What historical conclusions might you be able to draw? Perform the textual analysis and then write up your findings for Dr. Seefeldt. What might he be able to draw from this analysis? I will then take your work, compile it, and share it with him. Here are 3 research questions to get started. You can use one of these or come up with your own. 

* Is there a difference in promotion language vs. reception language. In other words, what was BBWW selling and what did Indiana newspapers find remarkable.
* Can we detect some change over time in promotion language and reception language (marketing and reviewing)? Were there types of characters, scenarios, action promised in promotional material and/or noted in reviews earlier vs later?
* What can be gleaned from the items tagged as extraneous as far as topics? These are news items that are somehow related to BBWW. Crime, finances, celebrity, etc.


Write up a research plan in the space below. What are you looking to explore?
>I will look at the extraneous words. First I need to identify what those words are and then see how they relate to one another and Buffalo Bill, if possible.

Lets download the data. 
```{r}
#| eval: false
#zip file of all the .txt files. One for each issue. 
download.file("https://github.com/dseefeldt/IndianaNewspapers/raw/main/bb-txt.zip", "bb-txt.zip")
unzip("bb-txt.zip")

# Metadata that includes info about each article.
bb.metadata <- read.csv("https://raw.githubusercontent.com/dseefeldt/IndianaNewspapers/main/metadata.csv")
```

(@) There are a few problems with this metadata from a tidy data perspective. First the date column isn't useful for assessing change over time. Can you split it into 3 columns one for year, month, and day? This will be useful if you want to try and identify change over time. Use the Date column not the FullDate column. This column is messy and you'll need to clean it up in order to have useful data. 
```{r}
bb.tidy <- bb.metadata %>% separate(Date, c("Month", "Day", "Year"))

print (bb.tidy)
```

(@) Now lets load the BB data. This happens in a slightly different way than with the SOTU data since its local data and not data from an R package. 
```{r}
data_dir <- paste(getwd(), "/txt", sep = "")
bb <- readtext(paste0(data_dir, "/*.txt"))
```

(@) Now you need to join this with the metadata for each file. Note that doc_id and Filename are the same value. How would you join two datasets together in R so that you can proceed with textual analysis? 
```{r}
bb.whole <- 
  bb.tidy %>%  
  arrange(Publication) %>% # sorts tidy version of bb.metadata (date seperated)
  bind_cols(bb) %>% # combines with full texts from bb
  as_tibble() 

glimpse(bb.whole)
```

(@) Now you can begin your textual analysis for Dr. S. Add code chunks below and intersperse text to explain what you are doing and why in the text of the worksheet.

```{r}
#filter the df to show 'E' (extraneous) items
bb.extra <- bb.whole %>%
  filter(R_P_E == "E")

bb.extra
```

```{r}
#find term frequency for words appearing in bb.extra

bb.extra.tf <- bb.extra %>%
  unnest_tokens(word, text)  %>% 
  count(word, sort = TRUE)  %>%
  group_by(word) %>%
  anti_join(stop_words) %>%

print(bb.extra.tf)
```

```{r}
#apply tf-idf to bb.extra

bb.extra.words <- bb.extra %>%
  unnest_tokens(word, text) %>%
  count(Filename, word, sort = TRUE) %>%
  ungroup()

bb.extra.total.words <- bb.extra.words %>%
  group_by(Filename) %>%
  summarize(total=sum(n))

bb.extra.words <- left_join(bb.extra.words, bb.extra.total.words)

bb.extra.words

bb.extra.freq.by.rank <- bb.extra.words %>%
  group_by(Filename) %>%
  mutate(rank = row_number(), 'term frequency'=n/total)
bb.extra.freq.by.rank

bb.extra.words <- bb.extra.words %>%
  bind_tf_idf(word, Filename, n) %>%
  anti_join(stop_words) %>%
  arrange(desc(tf_idf))
  
bb.extra.words


```

```{r}
bb.extra.words.top <- bb.extra.words %>%
    group_by(Filename) %>%
    top_n(1) %>%  # display the top tf-idf ranked word for each publication
  print(n = Inf)
```
>Not sure the above did what I wanted it to do

```{r}
bb.extra.pairs <- bb.extra.words %>% 
  #mutate(article_end = word(text, -5000, end = -1)) %>%  # extract last 100 words
  #unnest_tokens(word, article_end) %>%   # tokenize
  filter(!word %in% stop_words$word) %>%
   filter(bb.extra.words$word != "bill" & bb.extra.words$word != "buffalo" & bb.extra.words$word != "bill's" & bb.extra.words$word != "cody" & bb.extra.words$word != "pg" & bb.extra.words$word != "wild" & bb.extra.words$word != "west") %>% # remove additional stopwords
  pairwise_count(word, Filename, sort = TRUE, upper = FALSE) # don't include upper triangle of matrix

head(bb.extra.pairs)
```


```{r}
library(igraph)
library(ggraph)

bb.extra.pairs %>% 
  filter(n >= 20) %>%
  graph_from_data_frame() %>% #convert to graph
  ggraph(layout = "fr") + # place nodes according to the force-directed algorithm of Fruchterman and Reingold
  geom_edge_link(aes(edge_alpha = n, edge_width = n), edge_colour = "tomato") +
  geom_node_point(size = 5) +
  geom_node_text(aes(label = name), repel = TRUE, 
                 point.padding = unit(0.2, "lines")) +
  theme_void()
```
Write up a research plan in the space below. What are you looking to explore?
> If the articles labelled as 'extraneous' would reveal a different aspect of the BBWW show than what would be expected from the promotional and reception articles. I was hopeful from the tf-idf that something would develop around the word 'critic'. It showed up twice in the top 10 list and judging from some of the other high ranking words I thought perhaps that these articles were going to be comprised of more negative views of the show. But none of those words appeared in the co-occurrence graph, so I'm still unsure what implications can be drawn.

Below is what I originally submitted before I remember we were supposed to be answering one of your questions:

Write up a research plan in the space below. What are you looking to explore?

> Is there a way to identify the towns/regions of Indiana that identify more as frontier areas than others based on the way that Buffalo Bill's show was advertised? For instance, are "western" words (ie: indian, indians, riders, cowboys) more likely to be used in rural areas or areas on the borders or toward the western part of the state?
I will attempt to plot the frequency of the words printed according to the location of the newspaper and see if any meaningful conclusions can be drawn.

```{r}
bb.words <- bb.whole %>%
  unnest_tokens(word, text) %>% #unnests texts by single word
  anti_join(stop_words) %>% #removes standard stop words
  count(word, sort = TRUE) %>% #counts occurrences of each word across entire corpus
  as_tibble()

print (bb.words)

```

```{r}

bb.words.by.newspaper <- bb.whole %>%
  group_by(Publication) %>%
  unnest_tokens(word, text) %>% #unnests texts by single word
  anti_join(stop_words) %>% #removes standard stop words
  count(word, sort = TRUE) %>% #counts occurrences of each word across entire corpus
  as_tibble()

print(bb.words.by.newspaper)
```

```{r}
#remove Buffalo Bill name words from df

filtered.bb.words <- bb.words.by.newspaper %>%
  filter(bb.words.by.newspaper$word != "bill" & bb.words.by.newspaper$word != "buffalo" & bb.words.by.newspaper$word != "bill's" & bb.words.by.newspaper$word != "cody") %>%
  as_tibble()

print (filtered.bb.words)
```

```{r}
bb.jitter.plot <- 
  ggplot(subset(filtered.bb.words, word == "cowboy" | word == "history" & n > 5)) + geom_jitter(aes(Publication, n, colour = word)) + theme(axis.text.x = element_text(angle=90))

bb.jitter.plot
```


(@) Finally, when you are all done. Write up your findings. What research question did you focus on and what did you learn? 

> Question: Did certain areas of Indiana advertise Buffalo Bill's show with a stronger emphasis on western pioneer themes? Answer: Maybe.
I would need to take this analysis futher in order to determine a quickest way to plot these results on a map of Indiana. It's difficult because of how the publication information is stored. Might have to be hand entered and then plotted? I'm also not sure this is the best way to go about determining which terms to use - I just picked "cowboy" and "history" because they aren't part of the name of the show and they might be linked to an affinity for pioneer lore, but that's just my opinion. Those terms may have meant something different to readers in Indiana in this time period.

>I attempted to use LDA topic modeling but Palmetto wouldn't run the tidy() function correctly.

>So, I'm going to try something else based on "Text Mining with R: A Tidy Approach" was written by Julia Silge and David Robinson. In Chpt 3 they discuss Zipf's Law which the following code should demonstrate in the Buffalo Bill corpus.

```{r}
#creates a df linking each occurence of a word, the number of times it appears in each publication and the number of times it appears across the entire corpus
  unnest_tokens(word, text) %>%
  count(Publication, word, sort = TRUE)

bb.all.words <- bb.publication.words %>%
  group_by(Publication) %>%
  summarize(total = sum(n))

bb.publication.words <-left_join(bb.publication.words, bb.all.words)

bb.publication.words
```


```{r}
#ranks each word in each publication
bb.words.by.rank <- bb.publication.words %>%
  group_by(Publication) %>%
  mutate(rank = row_number(), term_freq = n/total) %>%
  ungroup()

bb.words.by.rank
```

```{r}
#plots the rank of words (and removes stopwords)
bb.words.by.rank %>%
  anti_join(stop_words) %>%
  ggplot(aes(rank, term_freq, color = Publication)) +
  geom_line(linewidth = 1.1, alpha = 0.8, show.legend = FALSE) +
  scale_x_log10() +
  scale_y_log10()
```

> I think this indicates that the majority of the words that appear in each publication appear in them all. 

> Next, I'll apply the tf-idf model to see which words occur in fewer of the documents which would indicate a greater weight.

```{r}
bb.tf.idf <- bb.publication.words %>%
  bind_tf_idf(word, Publication, n)

bb.tf.idf
```

```{r}
bb.tf.idf %>%
  select(-total) %>%
  arrange(desc(tf_idf))
```

> Appears to indicate that the names of the towns in which these papers were published receive a high idf.

```{r}
library(forcats)

bb.tf.idf %>%
  group_by(Publication) %>%
  slice_max(tf_idf, n = 1) %>%
  ungroup() %>%
  ggplot(aes(tf_idf, fct_reorder(word, tf_idf), fill = Publication)) + 
  geom_col(show.legend = FALSE) +
  facet_wrap(~Publication, ncol = 2, scales = "free") +
  labs(x = "tf-idf", y = NULL)
```

> This visualization would be better suited to a corpus containing a fewer number of publications

>The following code is my own based on the above results:

```{r}
#the top 5 ranked words by publication 
bb.top.ranked.word <- bb.tf.idf %>%
  group_by(Publication) %>%
  slice_max(tf_idf, n = 5)

bb.top.ranked.word
```

```{r}
bb.top.ranked.word %>%
  graph_from_data_frame() %>%
  ggraph(layout = "fr") +
  geom_edge_link(aes(edge_alpha = n, edge_width = n), edge_colour = "tomato") +
  geom_node_point(size = 5) +
  geom_node_text(aes(label = name), repel = TRUE, 
                 point.padding = unit(0.2, "lines")) +
  theme_void()
```

>Needs work
